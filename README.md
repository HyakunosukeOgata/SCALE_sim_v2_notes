# SCALE_sim_v2_notes
Notes and hands-on for SCALE-Sim v2 (systolic array accelerator simulator).

## ðŸ“Œ What is SCALE-Sim v2?
[SCALE-Sim v2](https://github.com/scalesim-project/scale-sim-v2) is a simulator for Systolic Array-based deep learning accelerators.  
It supports CNNs, feed-forward layers, and any workload that can be expressed as GEMM (General Matrix Multiplication) operations.  
It is useful for analyzing hardware performance and memory bandwidth requirements.

## âš™ Installation and Basic Usage
SCALE-Sim v2 is written entirely in Python. You can install it via `pip` or run it directly from the source.

### Method 1: Install via pip
```bash
pip3 install scalesim
```

### Method 2: Install from source
git clone https://github.com/scalesim-project/scale-sim-v2.git
cd scale-sim-v2
python3 setup.py install

â–¶ Running a Simulation
SCALE-Sim requires two input files:

Config file â€“ specifies architecture parameters and simulation settings.

Topology file â€“ describes the workload (e.g., CNN layers or GEMM parameters).

Example:
```bash
python3 scale.py -c configs/eyeriss.cfg -t topologies/alexnet.csv -p ./outputs
```
The output directory will include:

COMPUTE_REPORT.csv â€“ compute cycles, stall cycles, utilization percentages.

BANDWIDTH_REPORT.csv â€“ average and peak SRAM/DRAM bandwidth usage.

DETAILED_ACCESS_REPORT.csv â€“ access counts and cycles for each operand in SRAM and DRAM.

## ðŸ§ª My Observations and Experience

**Experiment setup:**
- **Array size:** High (weight = 255 Ã— 255)
- **SRAM sizes:** IFMAP = 8192 MB, FILTER = 8192 MB, OFMAP = 8192 MB
- **Memory offsets:** IFMAP = 0, FILTER = 10,000,000, OFMAP = 20,000,000
- **Dataflow:** WS (Weight Stationary)
- **Bandwidth:** 34
- **Memory banks:** 4
- **Interface bandwidth mode:** CALC

---

### ðŸ“Š MobileNet (224Ã—224 input)

**Performance Summary**
| LayerID | Total Cycles | Stall Cycles | Overall Util % | Mapping Efficiency % | Compute Util % |
|---------|--------------|--------------|----------------|----------------------|----------------|
| 0 | 13309 | 0 | 1.2426 | 1.3184 | 1.2191 |
| 1 | 25731 | 0 | 0.2067 | 0.2197 | 0.2026 |
| 2 | 13309 | 0 | 2.9454 | 3.1250 | 2.8898 |
| 3 | 11705 | 0 | 0.2355 | 0.2930 | 0.2210 |
| 4 | 3901  | 0 | 10.0487 | 12.5000 | 9.4299 |

**Bandwidth Usage**
| LayerID | Avg IFMAP SRAM BW | Avg FILTER SRAM BW | Avg OFMAP SRAM BW | Avg IFMAP DRAM BW | Avg FILTER DRAM BW | Avg OFMAP DRAM BW |
|---------|-------------------|--------------------|-------------------|-------------------|--------------------|-------------------|
| 0 | 25.4480 | 0.0649 | 30.1606 | 8.9717 | 0.1030 | 256.0000 |
| 1 | 135.4320 | 0.0112 | 0.9405 | 9.5703 | 0.0343 | 254.7368 |
| 2 | 30.1606 | 0.1539 | 60.3213 | 9.5703 | 0.2441 | 256.0000 |
| 3 | 154.3217 | 0.0492 | 0.8038 | 9.5703 | 0.0687 | 254.2703 |
| 4 | 51.4494 | 2.1000 | 102.8987 | 7.9752 | 0.9765 | 256.0000 |

**Layer Configurations**
| LayerID | Layer name | IFMAP Height | IFMAP Width | Filter Height | Filter Width | Channels | Num Filter | Strides |
|---------|------------|--------------|-------------|---------------|--------------|----------|------------|---------|
| 0 | Conv1 | 224 | 224 | 3 | 3 | 3 | 32 | 2 |
| 1 | Conv2 | 112 | 112 | 3 | 3 | 32 | 1 | 1 |
| 2 | Conv3 | 112 | 112 | 1 | 1 | 32 | 64 | 1 |
| 3 | Conv4 | 112 | 112 | 3 | 3 | 64 | 1 | 2 |
| 4 | Conv5 | 56  | 56  | 1 | 1 | 64 | 128 | 1 |

---

### ðŸ“Š GPT-2 Model Components

**Performance Summary**
| LayerID | Total Cycles | Stall Cycles | Overall Util % | Mapping Efficiency % | Compute Util % |
|---------|--------------|--------------|----------------|----------------------|----------------|
| 0 | 238069 | 0 | 50.4056 | 88.1109 | 44.1201 |
| 1 | 7159   | 0 | 14.3037 | 25.0000 | 12.5183 |
| 2 | 7159   | 0 | 14.3037 | 25.0000 | 12.5183 |
| 3 | 87709  | 0 | 45.6054 | 79.7194 | 39.9182 |
| 4 | 150359 | 0 | 51.0778 | 89.2857 | 44.7083 |

**Bandwidth Usage**
| LayerID | Avg IFMAP SRAM BW | Avg FILTER SRAM BW | Avg OFMAP SRAM BW | Avg IFMAP DRAM BW | Avg FILTER DRAM BW | Avg OFMAP DRAM BW |
|---------|-------------------|--------------------|-------------------|-------------------|--------------------|-------------------|
| 0 | 130.7587 | 32.2596 | 144.5228 | 9.7656 | 14.0492 | 162.1067 |
| 1 | 36.6174  | 9.1544  | 146.4696 | 7.8121 | 7.8121  | 256.0000 |
| 2 | 146.4696 | 9.1544  | 36.6174  | 9.6154 | 7.8121  | 256.0000 |
| 3 | 130.7597 | 29.1874 | 130.7597 | 9.7656 | 9.8444  | 162.4719 |
| 4 | 130.7591 | 32.6898 | 146.4501 | 9.7656 | 8.9915  | 175.7340 |

**Layer Configurations**
| Layer name | IFMAP Height | IFMAP Width | Filter Height | Filter Width | Channels | Num Filter | Strides |
|------------|--------------|-------------|---------------|--------------|----------|------------|---------|
| Linear1    | 1024 | 1600 | 1 | 1600 | 1 | 4800  | 1 |
| QKT        | 1024 | 64   | 1 | 64   | 1 | 1024  | 1 |
| QKTV       | 1024 | 1024 | 1 | 1024 | 1 | 64    | 1 |
| Linear2    | 1024 | 1600 | 1 | 1600 | 1 | 1600  | 1 |
| PW-FF-L1   | 1024 | 1600 | 1 | 1600 | 1 | 3072  | 1 |

---
## ðŸ§© Design Suggestions from This Report (PE Array / SRAM / Workload Fit)

This section provides actionable recommendations for **PE array size & aspect ratio**, **SRAM sizing & partitioning**, and **workload suitability**, based on the observed results (low utilization in MobileNet, ~50% utilization in GPT-2, sustained OFMAP DRAM saturation at 256 in MobileNet).

---

### 1) PE Array: Size & Aspect Ratio
**Observation link**: MobileNet layers are dominated by depthwise separable and 1Ã—1 convolutions â†’ extremely low utilization; GPT-2 large GEMM layers achieve ~50% utilization.

- **For CNN/Depthwise (MobileNet)**  
  - Use a **smaller or partitionable array** to avoid massive PE idling when the number of channels per layer is small.  
    Example: reduce from 255Ã—255 to **128Ã—128** or **64Ã—128**, or enable **multi-partition mode** to activate only part of the array dynamically.
  - Support **non-square (rectangular)** shapes: for 1Ã—1 or low-channel layers, shapes like **64Ã—256** or **128Ã—256** can improve mapping efficiency by stretching spatial tiling along the longer dimension.
  - **Per-layer dataflow switching**: for depthwise, use OS/RS (Output-Stationary / Row-Stationary) to reduce OFMAP write-backs.

- **For Transformer/GEMM (GPT-2)**  
  - Match **matrix aspect ratio** with **configurable array shapes**:  
    - tall-skinny (Mâ‰«N) â†’ **256Ã—128**  
    - wide-short (Nâ‰«M) â†’ **128Ã—256**  
    - near-square â†’ **256Ã—256** or **192Ã—192**
  - If external bandwidth is not a bottleneck, **increase total PE count** beyond 255Ã—255 â€” but verify DRAM/NoC throughput.

> TL;DR: CNNs benefit from â€œpartitionable + rectangularâ€ arrays for small-channel layers; GEMM prefers â€œlarge + aspect-ratio matchedâ€ arrays.

---

### 2) SRAM: Capacity & Partitioning
**Observation link**: In MobileNet, **Avg OFMAP DRAM BW = 256** is saturated across many layers â†’ output write-backs are the main bottleneck.

- **Prioritize expanding OFMAP SRAM (with double-buffering)**:  
  Goal: keep partial sums on-chip longer, reducing OFMAP write-back frequency.  
  Recommendation: given IFMAP/FILTER are both 8192 MB, **rebalance more capacity to OFMAP** if total SRAM budget cannot grow.
- **Increase SRAM bank count and multi-porting**:  
  Reduce bank conflicts and prevent on-chip write stalls.
- **Enable compression and write-combining**:  
  Apply zero compression or bit-width compression to OFMAP before DRAM write, and coalesce small writes at the DMA interface.

> Rule of thumb: if **Avg OFMAP DRAM BW** is persistently maxed out, first expand OFMAP SRAM/double-buffer, then consider increasing interface bandwidth.

---

### 3) Workload Suitability
- **Large GEMM / Transformer (GPT-2)**:  
  Well-suited for **large arrays** with WS/IS dataflows; utilization improves when array shape matches the matrix aspect ratio.  
  Once mapping efficiency is already high (~80â€“90%), check if **interface/DRAM/NoC** become the next limiting factor.
- **Depthwise / 1Ã—1-heavy CNN (MobileNet)**:  
  Best served by **partitionable or smaller arrays**, OS/RS dataflows, and **larger OFMAP SRAM**. Otherwise, expect simultaneous **OFMAP DRAM saturation** and **PE idling**.

---

### 4) Quantitative Validation Plan (Sweep in SCALE-Sim)
To quantify design trade-offs, run the following sweeps:

1. **Array shapes & sizes**:  
   `64Ã—128, 128Ã—128, 128Ã—256, 192Ã—192, 256Ã—256` (with partitioning on/off).
2. **Dataflow**:  
   MobileNet â†’ try OS/RS; GPT-2 â†’ try WS/IS (per-layer switching).
3. **SRAM allocation**:  
   Keep total size constant, shift more to OFMAP; enable double-buffering.
4. **Compression & precision**:  
   Compare Avg OFMAP DRAM BW and Total Cycles before/after enabling compression.
5. **Interface bandwidth ceiling**:  
   From `256 â†’ 384/512` to see if MobileNet escapes bandwidth bottlenecks, while checking if GPT-2 remains compute-bound.

**Success criteria**:  
- MobileNet: Avg OFMAP DRAM BW no longer pinned at 256; Overall Util improves.  
- GPT-2: Maintain high Mapping Efficiency while increasing Compute Util, without introducing new bandwidth bottlenecks.
